{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99282e46-4220-4df5-a08c-1db2519506fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from transformers import AutoTokenizer, CLIPVisionModel\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "from lit_llama.tokenizer import Tokenizer\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# process question when input model\n",
    "def pre_question(question, max_ques_words):\n",
    "    question = re.sub(\n",
    "        r\"([,.'!?\\\"()*#:;~])\",\n",
    "        '',\n",
    "        question.lower(),\n",
    "    ).replace(' \\t', ' ').replace('is/are', 'is').replace('near/in', 'in')\n",
    "    question = question.replace('>', 'more than ').replace('-yes/no', '')\n",
    "    question = question.replace('x ray', 'xray').replace('x-ray', 'xray')\n",
    "    question = question.rstrip(' ')\n",
    "\n",
    "    # truncate question\n",
    "    question_words = question.split(' ')\n",
    "    if len(question_words) > max_ques_words:\n",
    "        question = ' '.join(question_words[:max_ques_words])\n",
    "\n",
    "    return question\n",
    "\n",
    "# process answer when input model\n",
    "def pre_answer(answer):\n",
    "    answer = str(answer)\n",
    "    answer = re.sub(\n",
    "        r\"([,.'!?\\\"()*#:;~])\",\n",
    "        '',\n",
    "        answer.lower(),\n",
    "    ).replace(' \\t', ' ')\n",
    "    answer = answer.replace('x ray', 'xray').replace('x-ray', 'xray')\n",
    "    answer = answer.replace(' - ', '-')\n",
    "    return answer\n",
    "\n",
    "def visual_feature(model, image, proj):\n",
    "    inputs_img = {'pixel_values':0}\n",
    "    inputs_img['pixel_values'] = image.unsqueeze(0)\n",
    "    \n",
    "    #image feature\n",
    "    image_features = model(**inputs_img)\n",
    "    selected_image_features = image_features.last_hidden_state[:, 1:]\n",
    "\n",
    "    #feature embedding\n",
    "    img_emd = projector(selected_image_features)\n",
    "    return img_emd\n",
    "    \n",
    "\n",
    "class LlavaMultiModalProjector(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(config[\"hidden_size\"], config[\"hidden_size_text\"], bias=True)\n",
    "        self.act = ACT2FN[config[\"projector_hidden_act\"]]\n",
    "        self.linear_2 = nn.Linear(config[\"hidden_size_text\"], config[\"hidden_size_text\"], bias=True)\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        hidden_states = self.linear_1(image_features)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.linear_2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
    "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
    "\n",
    "\n",
    "def generate_prompt_qa(item, content):\n",
    "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
    "    'response' field.\"\"\"\n",
    "    if item == \"question\": \n",
    "        return f\"Context:\\nImage\\nQuestion:\\n{content}\\nAnswer:\\n\"\n",
    "    else:\n",
    "        return f\"Answer:\\n{content}\"\n",
    "\n",
    "\n",
    "\n",
    "class vqa_dataset(Dataset):\n",
    "    def __init__(self, ann_file, transform, vqa_root, eos='[SEP]', max_ques_words=100, max_length = 512,\n",
    "                 answer_list='', clip_model = None, feature_proj=None, tokenizer = None):\n",
    "        \n",
    "        self.ann =  pd.read_pickle(ann_file)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.vqa_root = vqa_root\n",
    "        self.max_ques_words = max_ques_words\n",
    "        self.eos = eos\n",
    "\n",
    "        self.clip_model = clip_model\n",
    "        self.feature_proj = feature_proj\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ann = self.ann[index]\n",
    "\n",
    "        image_path = os.path.join(self.vqa_root, ann['image'])\n",
    "        image = Image.open(image_path + '.jpg').convert('RGB')\n",
    "        image = self.transform(image).cuda()\n",
    "        \n",
    "        image_proj = visual_feature(self.clip_model, image, self.feature_proj)\n",
    "        question = pre_question(ann['question'], self.max_ques_words)\n",
    "        answers = ann['answer']\n",
    "        answers = pre_answer(answers)\n",
    "\n",
    "        full_prompt = generate_prompt_qa('question',question)\n",
    "        full_prompt_and_response = full_prompt + answers\n",
    "        encoded_full_prompt = tokenize(self.tokenizer, full_prompt, max_length=self.max_length, eos=False)\n",
    "        encoded_full_prompt_and_response = tokenize(self.tokenizer, full_prompt_and_response, eos=True, max_length=self.max_length)\n",
    "        label = encoded_full_prompt_and_response.clone()\n",
    "        label[:len(encoded_full_prompt)] = -1\n",
    "        \n",
    "        \n",
    "        if len(encoded_full_prompt_and_response)==self.max_length:\n",
    "            return None\n",
    "\n",
    "        return {'context':image_proj, 'question':question, 'answer':answers, 'qa_token': encoded_full_prompt_and_response, 'tokens': full_prompt_and_response, 'q_token': encoded_full_prompt, 'label': label}\n",
    "\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "train_transform = transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(384, scale=(0.5, 1.0), interpolation=Image.BICUBIC),\n",
    "        transforms.Resize((224,224), interpolation=Image.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # RandomAugment(2, 7, isPIL=True, augs=['Identity', 'AutoContrast', 'Equalize', 'Brightness', 'Sharpness',\n",
    "        #                                       'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "train_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/qas/train/train_qa.pkl'\n",
    "val_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/qas/val/val_qa.pkl'\n",
    "test_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/qas/test/test_qa.pkl'\n",
    "\n",
    "train_image_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/images/train'\n",
    "val_image_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/images/val'\n",
    "test_image_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/images/test'\n",
    "\n",
    "config = {}\n",
    "config[\"hidden_size\"] = 768\n",
    "config[\"hidden_size_text\"] = 4096\n",
    "config[\"projector_hidden_act\"] = 'gelu'\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch16\").cuda()\n",
    "model.eval()\n",
    "projector = LlavaMultiModalProjector(config).cuda()\n",
    "\n",
    "tokenizer_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "train_dataset = vqa_dataset(train_path, train_transform, train_image_path, clip_model = model, feature_proj = projector, tokenizer = tokenizer)\n",
    "val_dataset = vqa_dataset(val_path, train_transform, val_image_path, clip_model = model, feature_proj = projector, tokenizer = tokenizer)\n",
    "test_dataset = vqa_dataset(test_path, train_transform, test_image_path, clip_model = model, feature_proj = projector, tokenizer = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed4e1e1-e5ab-4015-8edf-46fc52c46e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 4096]),\n",
       " dict_keys(['context', 'question', 'answer', 'qa_token', 'tokens', 'q_token', 'label']),\n",
       " 'where are liver stem cells oval cells located',\n",
       " 'in the canals of hering',\n",
       " 19755,\n",
       " tensor([    1, 22430, 31871,    13,  9895,    13,  6347, 28312, 31871,    13,\n",
       "          3272,   397, 13357, 10700,  3984,   269,  1735,  3984,  3521,    13,\n",
       "          5092,  2055,   265, 31871,    13,   261,   266,   473,   811,   287,\n",
       "           600,   281,     2], dtype=torch.int32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['context'].size(), train_dataset[0].keys(), train_dataset[0]['question'], train_dataset[0]['answer'], len(train_dataset), train_dataset[0]['qa_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a00110-666e-41a0-8894-3dcea44bb887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 4096]),\n",
       " dict_keys(['context', 'question', 'answer', 'qa_token', 'tokens', 'q_token', 'label']),\n",
       " 'what have lost their nuclei',\n",
       " 'neutrophils',\n",
       " 6279,\n",
       " tensor([    1, 22430, 31871,    13,  9895,    13,  6347, 28312, 31871,    13,\n",
       "         11906,   435,  2953,   518, 13323, 31827,    13,  5092,  2055,   265,\n",
       "         31871,    13,   652,   319,  8982,  3640,     2], dtype=torch.int32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]['context'].size(), val_dataset[0].keys(), val_dataset[0]['question'], val_dataset[0]['answer'], len(val_dataset), val_dataset[0]['qa_token']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7752b399-bf18-47cb-82e7-d7e73acc0dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 4096]),\n",
       " dict_keys(['context', 'question', 'answer', 'qa_token', 'tokens', 'q_token', 'label']),\n",
       " 'what are positively charged  thus allowing the compaction of the negatively charged dna',\n",
       " 'the histone subunits',\n",
       " 6761,\n",
       " tensor([    1, 22430, 31871,    13,  9895,    13,  6347, 28312, 31871,    13,\n",
       "         11906,   397, 18807,  6448,  4487,  5905,   266,   520,  2794,   287,\n",
       "           266, 23162,  6448,   294,  2244,    13,  5092,  2055,   265, 31871,\n",
       "            13,  1134,  1285,   534,   845,   356,   916,     2],\n",
       "        dtype=torch.int32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['context'].size(), test_dataset[0].keys(),test_dataset[0]['question'], test_dataset[0]['answer'], len(test_dataset), test_dataset[0]['qa_token']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0713f124-7dae-45e2-92ae-2a516bff64f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Context:\\nImage\\nQuestion:\\nwhere are liver stem cells oval cells located\\nAnswer:\\nin the canals of hering',\n",
       " tensor([    1, 22430, 31871,    13,  9895,    13,  6347, 28312, 31871,    13,\n",
       "          3272,   397, 13357, 10700,  3984,   269,  1735,  3984,  3521,    13,\n",
       "          5092,  2055,   265, 31871,    13], dtype=torch.int32),\n",
       " tensor([    1, 22430, 31871,    13,  9895,    13,  6347, 28312, 31871,    13,\n",
       "          3272,   397, 13357, 10700,  3984,   269,  1735,  3984,  3521,    13,\n",
       "          5092,  2055,   265, 31871,    13,   261,   266,   473,   811,   287,\n",
       "           600,   281,     2], dtype=torch.int32),\n",
       " tensor([ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 261, 266, 473,\n",
       "         811, 287, 600, 281,   2], dtype=torch.int32),\n",
       " torch.Size([33]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['tokens'] , train_dataset[0]['q_token'], train_dataset[0]['qa_token'], train_dataset[0]['label'], train_dataset[0]['label'].size()\n",
    "#  1, 22430, 31871,    13, 31903,  8326, 31901,    13,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d106d0c7-5c3c-441b-897e-f73b418b7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ix tensor([ 6406, 12253])\n",
      "input_ids 2 2\n",
      "max_len 24\n",
      "emb_x torch.Size([2, 24, 4096])\n",
      "image_features torch.Size([2, 196, 4096]) <class 'torch.Tensor'>\n",
      "2 196 4096\n",
      "2 24\n",
      "num_special_image_tokens tensor([1, 1])\n",
      "max_embed_dim tensor(219)\n",
      "batch_indices, non_image_indices tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) tensor([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23,  0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "        14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "new_token_positions tensor([[  0,   1,   2,   3, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "         209, 210, 211, 212, 213, 214, 215, 216, 217, 218],\n",
      "        [  0,   1,   2,   3, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "         209, 210, 211, 212, 213, 214, 215, 216, 217, 218]])\n",
      "nb_image_pad tensor([0, 0], device='cuda:0')\n",
      "text_to_overwrite torch.Size([46])\n",
      "final_embedding[batch_indices, text_to_overwrite] torch.Size([46, 4096])\n",
      "image_to_overwrite torch.Size([2, 219])\n",
      "final_embedding[image_to_overwrite] torch.Size([392, 4096])\n",
      "final_embedding torch.Size([2, 219, 4096])\n",
      "temp_ebd_label torch.Size([2, 219])\n",
      "ids tensor(6406)\n",
      "ids tensor(12253)\n",
      "labels_ torch.Size([2, 219]) tensor([[  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,  431,\n",
      "         2631,  467,    2],\n",
      "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "           -1, 3976,    2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from lit_llama.tokenizer import Tokenizer\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "config[\"padded_vocab_size\"]= 32000\n",
    "config[\"n_embd\"] = 4096\n",
    "\n",
    "# the 4th position of qa_token, with the position of text \"Image\"\n",
    "config[\"image_token_index\"] = 9895 \n",
    "micro_batch_size = 2\n",
    "IGNORE_INDEX = -1\n",
    "\n",
    "def prepare_sample(example: dict, max_length: int, mask_inputs: bool = True):\n",
    "\n",
    "    ix = torch.randint(len(example), (micro_batch_size,))\n",
    "    print(\"ix\", ix)\n",
    "\n",
    "    input_ids = [example[i][\"qa_token\"].type(torch.int64) for i in ix]\n",
    "    # input_ids = torch.stack([example[i][\"qa_token\"].type(torch.int64) for i in ix])\n",
    "    labels = [example[i][\"label\"] for i in ix]\n",
    "    print('input_ids', len(input_ids), len(labels))\n",
    "\n",
    "    max_len = max(len(s) for s in input_ids)\n",
    "    print('max_len', max_len)\n",
    "\n",
    "    def pad_right(x, pad_id):\n",
    "        # pad right based on the longest sequence\n",
    "        n = max_len - len(x)\n",
    "        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n",
    "\n",
    "    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n",
    "    # y = torch.stack([pad_right(y, pad_id=-1) for y in labels])\n",
    "    # print(\"xy\", x.size(), y.size())\n",
    "\n",
    "    wte=nn.Embedding(config['padded_vocab_size'], config['n_embd'])\n",
    "    text_ebd = wte(x)\n",
    "    print(\"emb_x\", text_ebd.size())\n",
    "\n",
    "    # image_features = [example[i][\"context\"] for i in ix]\n",
    "    image_features = torch.stack([example[i][\"context\"].squeeze(0) for i in ix])\n",
    "    print(\"image_features\", image_features.size(), type(image_features))\n",
    "\n",
    "    num_images, num_image_patches, embed_dim = image_features.shape\n",
    "    print(num_images, num_image_patches, embed_dim)\n",
    "\n",
    "    batch_size, sequence_length = x.shape\n",
    "    print(batch_size, sequence_length)\n",
    "\n",
    "    special_image_token_mask = x == config[\"image_token_index\"]\n",
    "    # print(\"x\", x)\n",
    "    # print(\"special_image_token_mask\", special_image_token_mask)\n",
    "\n",
    "    num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n",
    "    print(\"num_special_image_tokens\", num_special_image_tokens)\n",
    "\n",
    "    max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + x.size(1)\n",
    "    print(\"max_embed_dim\", max_embed_dim)\n",
    "\n",
    "    batch_indices, non_image_indices = torch.where(x != config[\"image_token_index\"])\n",
    "    print(\"batch_indices, non_image_indices\", batch_indices, non_image_indices)\n",
    "\n",
    "    new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n",
    "    print(\"new_token_positions\", new_token_positions)\n",
    "\n",
    "    nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n",
    "    nb_image_pad = nb_image_pad.cuda()\n",
    "    print(\"nb_image_pad\", nb_image_pad)\n",
    "    \n",
    "    text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n",
    "    print(\"text_to_overwrite\", text_to_overwrite.size())\n",
    "\n",
    "    final_embedding = torch.zeros(\n",
    "        batch_size, max_embed_dim, embed_dim, dtype=image_features.dtype, device=image_features.device)\n",
    "\n",
    "    final_embedding[batch_indices, text_to_overwrite] = text_ebd[batch_indices, non_image_indices].cuda()\n",
    "    print('final_embedding[batch_indices, text_to_overwrite]', final_embedding[batch_indices, text_to_overwrite].size())\n",
    "\n",
    "    image_to_overwrite = torch.all(final_embedding == 0, dim=-1)\n",
    "    print(\"image_to_overwrite\", image_to_overwrite.size())\n",
    "    \n",
    "    image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None]\n",
    "    # print(\"image_to_overwrite1\", image_to_overwrite)\n",
    "\n",
    "    if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n",
    "        raise ValueError(\n",
    "            f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n",
    "            f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n",
    "        )\n",
    "\n",
    "    final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim)\n",
    "    print(\"final_embedding[image_to_overwrite]\", final_embedding[image_to_overwrite].size())\n",
    "\n",
    "    print(\"final_embedding\", final_embedding.size())\n",
    "\n",
    "    #getting labels\n",
    "    temp_ebd_label = torch.zeros(batch_size, max_embed_dim, device=image_features.device).long() - 1\n",
    "    print(\"temp_ebd_label\", temp_ebd_label.size())\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(ix)):\n",
    "        ids = ix[i]\n",
    "        print(\"ids\", ids)\n",
    "        temp_len = len(example[ids][\"label\"])\n",
    "        temp_ebd_label[i][-temp_len:] = example[ids][\"label\"]\n",
    "        labels.append(temp_ebd_label[i])\n",
    "    labels_ = torch.stack([y for y in labels])\n",
    "    print(\"labels_\", labels_.size(), labels_)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    return { \"input_ids\": final_embedding, \"labels\": labels_}\n",
    "\n",
    "sample_set = prepare_sample(example = train_dataset, max_length=512, mask_inputs= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89fd6866-ab0e-419b-be83-bd6e3cf83212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[ 0.9946,  0.2502, -0.1851,  ...,  0.0617, -0.4781, -1.6970],\n",
       "          [ 0.2183,  1.5907,  2.1551,  ...,  0.7390, -0.0546,  0.5157],\n",
       "          [-1.0250,  0.0686, -1.0518,  ..., -1.0820,  0.8993,  1.0964],\n",
       "          ...,\n",
       "          [ 1.3632, -1.1058, -0.6484,  ...,  1.0083, -0.6516, -0.7756],\n",
       "          [ 0.3430,  0.6047, -0.7093,  ...,  0.1265,  0.9999,  0.8103],\n",
       "          [-1.2783,  0.8782,  0.4749,  ..., -0.3862,  0.1837, -0.0648]],\n",
       " \n",
       "         [[ 0.9946,  0.2502, -0.1851,  ...,  0.0617, -0.4781, -1.6970],\n",
       "          [ 0.2183,  1.5907,  2.1551,  ...,  0.7390, -0.0546,  0.5157],\n",
       "          [-1.0250,  0.0686, -1.0518,  ..., -1.0820,  0.8993,  1.0964],\n",
       "          ...,\n",
       "          [-0.5672,  2.1402, -0.2583,  ..., -0.0036,  0.7955,  0.3287],\n",
       "          [ 1.1418, -0.8197,  1.1176,  ...,  0.1624,  0.0037, -1.6648],\n",
       "          [ 0.3430,  0.6047, -0.7093,  ...,  0.1265,  0.9999,  0.8103]]],\n",
       "        device='cuda:0', grad_fn=<IndexPutBackward0>),\n",
       " 'labels': tensor([[  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,  431,\n",
       "          2631,  467,    2],\n",
       "         [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "            -1, 3976,    2]], device='cuda:0')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ebf94c-ec42-402c-bbee-777e5d87e6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 261, 266, 473,\n",
       "        811, 287, 600, 281,   2], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels and keep other positions masked\n",
    "x = train_dataset[0]['q_token']\n",
    "y = train_dataset[0]['qa_token']\n",
    "y[:len(x)] = -1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13017fd0-45e8-430e-86d5-541bbf4a6735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, -1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "model_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "processor = SentencePieceProcessor(model_file=str(model_path))\n",
    "processor.bos_id(), processor.eos_id(), processor.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6bed5e-f6c2-4615-9f6e-0aa5216f8e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33ef23f-434b-439f-af04-ad787c1db29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_218437/2019754879.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(os.path.join(data_dir, \"train_data_fuse_v1.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " dict_keys(['context', 'question', 'answer', 'input_ids', 'labels']),\n",
       " torch.Size([1, 228, 4096]),\n",
       " torch.Size([1, 228]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "IGNORE_INDEX = -1\n",
    "tokenizer_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "training_file_path = Path(\"data/squad2\")\n",
    "train_sample_set = []\n",
    "\n",
    "for i in range(50):\n",
    "    sample_set = prepare_sample(train_dataset[i], tokenizer, max_length=512, mask_inputs= True)\n",
    "    if sample_set == None:\n",
    "        continue\n",
    "    else:\n",
    "        train_sample_set.append(sample_set)\n",
    "        i += 1\n",
    "    torch.save(train_sample_set, training_file_path.parent / \"train_data_fuse_v1.pt\") #\"/data/user-data/sa25729/lit_llama_qa/lit_llama_qa/data/train_temp.pt\"\n",
    "\n",
    "def load_datasets(data_dir):\n",
    "    train_data = torch.load(os.path.join(data_dir, \"train_data_fuse_v1.pt\"))\n",
    "    # val_data = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "    # return train_data, val_data\n",
    "    return train_data\n",
    "\n",
    "\n",
    "data_dir = \"/data/user-data/sa25729/lit_llama_qa/lit-llama-qa/data/\"\n",
    "train_data = load_datasets(data_dir)\n",
    "len(train_data), train_data[0].keys(), train_data[0]['input_ids'].size(), train_data[0]['labels'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f734513-0f31-4d48-b357-b143a6471e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 4096])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['context'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0055b4-f447-4435-a570-bffd84fbf6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab2adb-5b69-4090-9c65-ad5bc5210a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bd523-c2bf-4ef8-bfd0-3af6770b2ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f5ffc9c-a68d-493d-8067-0d22f4c33aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context:\\nimage\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lit_llama.tokenizer import Tokenizer\n",
    "\n",
    "temp = \"hello\"\n",
    "\n",
    "x = torch.tensor([  1, 22430, 31871,    13, 31903,  8326, 31901,    13,  6347, 28312,\n",
    "        31871,    13,  3272,   397, 13357, 10700,  3984,   269,  1735,  3984,\n",
    "         3521,    13,  5092,  2055,   265, 31871,    13,   261,   266,   473,\n",
    "          811,   287,   600,   281,     2])\n",
    "\n",
    "x_ = torch.tensor([  1, 22430, 31871,    13,   8326, 13])\n",
    "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
    "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
    "\n",
    "def tokenize_dec(tokenizer: Tokenizer, token: str):\n",
    "    return tokenizer.decode(token)\n",
    "    \n",
    "tokenize(tokenizer, temp, eos=True, max_length=50)\n",
    "tokenize_dec(tokenizer, x_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10fe8834-7ce7-4fca-ba51-2e2b7ce3fc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31903])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([  1, 22430, 31871,    13, 31903,  8326, 31901,    13,  6347, 28312,\n",
    "        31871,    13,  3272,   397, 13357, 10700,  3984,   269,  1735,  3984,\n",
    "         3521,    13,  5092,  2055,   265, 31871,    13,   261,   266,   473,\n",
    "          811,   287,   600,   281,     2]).unsqueeze(0)\n",
    "x[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120a95b5-e186-4466-b8b0-9fc99e9c77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cd7a0-3397-45b0-9dd5-0ae2ecb8c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    # wte=nn.Embedding(config['padded_vocab_size'], config['n_embd'])\n",
    "    # text_ebd = wte(encoded_full_prompt_and_response).unsqueeze(0)\n",
    "\n",
    "    # image_features = example[\"context\"]\n",
    "    # num_images, num_image_patches, embed_dim = image_features.shape \n",
    "\n",
    "    # batch_size, sequence_length = encoded_full_prompt_and_response.unsqueeze(0).shape\n",
    "\n",
    "    # encoded_full_prompt_and_response_ = encoded_full_prompt_and_response.unsqueeze(0)\n",
    "    \n",
    "    # image_token_index = encoded_full_prompt_and_response_[:,4]\n",
    "    \n",
    "    # special_image_token_mask = encoded_full_prompt_and_response_ == encoded_full_prompt_and_response_[:,4]\n",
    "\n",
    "    # num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n",
    "\n",
    "    # max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + encoded_full_prompt_and_response_.size(1)\n",
    "\n",
    "    # batch_indices, non_image_indices = torch.where(encoded_full_prompt_and_response_ != image_token_index)\n",
    "\n",
    "    # new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n",
    "\n",
    "    # nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n",
    "    # nb_image_pad = nb_image_pad.cuda()\n",
    "\n",
    "    # text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n",
    "\n",
    "    # final_embedding = torch.zeros(\n",
    "    #     batch_size, max_embed_dim, embed_dim, dtype=image_features.dtype, device=image_features.device)\n",
    "    \n",
    "    # final_embedding[batch_indices, text_to_overwrite] = text_ebd[batch_indices, non_image_indices].cuda()\n",
    "\n",
    "    # image_to_overwrite = torch.all(final_embedding == 0, dim=-1)\n",
    "    \n",
    "    # image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None]\n",
    "\n",
    "    # if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n",
    "    #     raise ValueError(\n",
    "    #         f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n",
    "    #         f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n",
    "    #     )\n",
    "\n",
    "    # final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
