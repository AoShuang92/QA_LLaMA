{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d88a997-b20d-45ae-b1f4-8e476b125456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 17:38:12.458547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 17:38:13.332765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/data/user-data/sa25729/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "\n",
    "\n",
    "# process question when input model\n",
    "def pre_question(question, max_ques_words):\n",
    "    question = re.sub(\n",
    "        r\"([,.'!?\\\"()*#:;~])\",\n",
    "        '',\n",
    "        question.lower(),\n",
    "    ).replace(' \\t', ' ').replace('is/are', 'is').replace('near/in', 'in')\n",
    "    question = question.replace('>', 'more than ').replace('-yes/no', '')\n",
    "    question = question.replace('x ray', 'xray').replace('x-ray', 'xray')\n",
    "    question = question.rstrip(' ')\n",
    "\n",
    "    # truncate question\n",
    "    question_words = question.split(' ')\n",
    "    if len(question_words) > max_ques_words:\n",
    "        question = ' '.join(question_words[:max_ques_words])\n",
    "\n",
    "    return question\n",
    "\n",
    "# process answer when input model\n",
    "def pre_answer(answer):\n",
    "    answer = str(answer)\n",
    "    answer = re.sub(\n",
    "        r\"([,.'!?\\\"()*#:;~])\",\n",
    "        '',\n",
    "        answer.lower(),\n",
    "    ).replace(' \\t', ' ')\n",
    "    answer = answer.replace('x ray', 'xray').replace('x-ray', 'xray')\n",
    "    answer = answer.replace(' - ', '-')\n",
    "    return answer\n",
    "\n",
    "def visual_feature(model, image, proj):\n",
    "    inputs_img = {'pixel_values':0}\n",
    "    inputs_img['pixel_values'] = image.unsqueeze(0)\n",
    "    image_features = model.get_image_features(**inputs_img)\n",
    "    image_features_proj = proj(image_features)\n",
    "    return image_features_proj\n",
    "    \n",
    "#reference of paper or github repo\n",
    "class img_projection(nn.Module):\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(img_projection, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(nn.Dropout(p=0.5))\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class vqa_dataset(Dataset):\n",
    "    def __init__(self, ann_file, transform, vqa_root, eos='[SEP]', split=\"train\", max_ques_words=30,\n",
    "                 answer_list='', clip_model = None, feature_proj=None):\n",
    "        self.split = split\n",
    "        self.ann =  pd.read_pickle(ann_file)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.vqa_root = vqa_root\n",
    "        self.max_ques_words = max_ques_words\n",
    "        self.eos = eos\n",
    "\n",
    "        self.clip_model = clip_model\n",
    "        self.feature_proj = feature_proj\n",
    "\n",
    "        if split == 'test':\n",
    "            self.max_ques_words = 50  # do not limit question length during test\n",
    "            self.answer_list = json.load(open(answer_list, 'r'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ann = self.ann[index]\n",
    "\n",
    "        image_path = os.path.join(self.vqa_root, ann['image'])\n",
    "        image = Image.open(image_path + '.jpg').convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        image_proj = visual_feature(self.clip_model, image, self.feature_proj)\n",
    "        # prompt_info = ', its organ is {}, the type of answer is {}, the type of question is {}'\\\n",
    "        #               .format(ann['image_organ'], ann['answer_type'], ann['question_type'])\n",
    "        # ann['question'] = ann['question'] + prompt_info\n",
    "\n",
    "\n",
    "        if self.split == 'test':\n",
    "            question = pre_question(ann['question'], self.max_ques_words)\n",
    "            question_id = ann['qid']\n",
    "            return image, question, question_id\n",
    "\n",
    "        elif self.split == 'train':\n",
    "\n",
    "            question = pre_question(ann['question'], self.max_ques_words)\n",
    "            answers = ann['answer']\n",
    "            answers = pre_answer(answers)\n",
    "            # answers = [pre_answer(answers)]\n",
    "            # answers = [answer + self.eos for answer in answers]\n",
    "\n",
    "            return {'context':image_proj, 'question':question, 'answer':answers}\n",
    "\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "train_transform = transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(384, scale=(0.5, 1.0), interpolation=Image.BICUBIC),\n",
    "        transforms.Resize((224,224), interpolation=Image.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # RandomAugment(2, 7, isPIL=True, augs=['Identity', 'AutoContrast', 'Equalize', 'Brightness', 'Sharpness',\n",
    "        #                                       'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "train_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/qas/train/train_qa.pkl'\n",
    "image_path = '/data/user-data/sa25729/MICCAI_2024/datasets/pvqa/images/train'\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "proj = img_projection((512, 128))\n",
    "\n",
    "train_dataset = vqa_dataset(train_path, train_transform, image_path, split='train', clip_model = model, feature_proj = proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b2c613-49fc-43a5-b4fd-fb84b4ea8e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]),\n",
       " {'context': tensor([[ 0.2929,  0.2182, -0.1688,  0.3002,  0.1204,  0.1557,  0.0088, -0.5786,\n",
       "           -0.3105,  0.3801,  0.3762, -0.2273, -0.2410,  0.2423,  0.5150,  0.4706,\n",
       "            0.3800, -0.0644,  0.0296, -0.3151, -0.0946, -0.0570,  0.0251, -0.0463,\n",
       "            0.2365,  0.0978, -0.6446, -0.0106, -0.2707, -0.2651,  0.2845,  0.2669,\n",
       "           -0.2838,  0.1725,  0.3792, -0.4023, -0.5583,  0.0692,  0.3028, -0.1613,\n",
       "            0.0449,  0.1432,  0.5284,  0.4707, -0.1834, -0.0068, -0.2882,  0.4106,\n",
       "            0.0736, -0.0572,  0.0292,  0.3399, -0.4908,  0.0587, -0.0130,  0.0515,\n",
       "           -0.2022, -0.2150, -0.2021, -0.0681,  0.4435, -0.1914, -0.0609,  0.2424,\n",
       "            0.1118, -0.1728, -0.3249, -0.1466, -0.2586,  0.1690,  0.1559, -0.3386,\n",
       "           -0.3448,  0.0263, -0.1364, -0.0922,  0.1560,  0.1121,  0.1112, -0.3842,\n",
       "            0.1458,  0.2866,  0.0873, -0.3112,  0.4269, -0.0166,  0.4459,  0.1424,\n",
       "           -0.1220, -0.2386, -0.5953,  0.0027,  0.3112, -0.1762, -0.1974, -0.2435,\n",
       "           -0.1033, -0.0602,  0.1466, -0.2365, -0.0513, -0.1836, -0.0342,  0.4528,\n",
       "            0.3313, -0.4469, -0.2211,  0.4837, -0.3657,  0.0484, -0.1998, -0.0787,\n",
       "            0.1618, -0.1633, -0.0547, -0.1276,  0.3046,  0.2694, -0.0237, -0.4094,\n",
       "           -0.1776, -0.1252,  0.1587, -0.1633,  0.5102,  0.2733,  0.2273, -0.2115]],\n",
       "         grad_fn=<AddmmBackward0>),\n",
       "  'question': 'where are liver stem cells oval cells located',\n",
       "  'answer': 'in the canals of hering'},\n",
       " str,\n",
       " str)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['context'].size(), train_dataset[0], type(train_dataset[0]['question']), type(train_dataset[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf15086-c98e-4a90-8eb1-c5f479b36b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_llama.tokenizer import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "def prepare_sample(example: dict, tokenizer: Tokenizer, max_length: int, mask_inputs: bool = True):\n",
    "    \n",
    "    full_prompt = generate_prompt_qa(example, item = 'question')\n",
    "    full_prompt_and_response = full_prompt + example[\"answer\"]\n",
    "    # print('full_prompt_and_response', full_prompt_and_response)\n",
    "    \n",
    "    # encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=max_length, eos=False)\n",
    "    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=max_length)\n",
    "    # print('encoded_full_prompt_and_response', encoded_full_prompt_and_response.size(), encoded_full_prompt_and_response)\n",
    "\n",
    "    ans = generate_prompt_qa(example, item = 'answer')\n",
    "    encoded_ans = tokenize(tokenizer, ans, eos=True, max_length=max_length)\n",
    "    # print('encoded_ans', encoded_ans, len(encoded_ans))\n",
    "    \n",
    "    #concating the img feature with tokens\n",
    "    x = example[\"context\"].transpose(0,1)\n",
    "    y = encoded_full_prompt_and_response.unsqueeze(0).transpose(0,1)\n",
    "    cqa = torch.cat((x, y), dim=0).transpose(0,1)\n",
    "    # print(x.size(), y.size(), cqa.size())\n",
    "    \n",
    "    if len(cqa)==max_length:\n",
    "        return None\n",
    "    # print(f\"Length of tokens: {len(encoded_full_prompt_and_response)}\")\n",
    "\n",
    "    # The labels are the full prompt with response, but with the prompt masked out\n",
    "    labels = cqa.clone()\n",
    "    \n",
    "    if mask_inputs:\n",
    "        labels[:, 0:-len(encoded_ans)] = IGNORE_INDEX\n",
    "\n",
    "    return {**example, \"input_ids\": cqa, \"labels\": labels}\n",
    "\n",
    "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
    "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
    "\n",
    "\n",
    "def generate_prompt_qa(example, item):\n",
    "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
    "    'response' field.\"\"\"\n",
    "    if item == \"question\": \n",
    "        return f\"### Question:\\n{example[item]}\\n\\n### Answer:\\n\"\n",
    "    else:\n",
    "        return f\"### Answer:\\n{example[item]}\"\n",
    "    # return f\"### Context:\\n{example['context']}\\n\\n### Question:\\n{example['question']}\\n\\n### Answer:\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd954af-48b1-442b-aaa9-e625bb492370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "IGNORE_INDEX = -1\n",
    "tokenizer_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "training_file_path = Path(\"data/squad2\")\n",
    "train_sample_set = []\n",
    "\n",
    "for i in range(50):\n",
    "    sample_set = prepare_sample(train_dataset[i], tokenizer, max_length=512, mask_inputs= True)\n",
    "    if sample_set == None:\n",
    "        continue\n",
    "    else:\n",
    "        train_sample_set.append(sample_set)\n",
    "        i += 1\n",
    "    torch.save(train_sample_set, training_file_path.parent / \"train_temp.pt\") #\"/data/user-data/sa25729/lit_llama_qa/lit_llama_qa/data/train_temp.pt\"\n",
    "\n",
    "# for sample in tqdm(train_dataset):\n",
    "#     sample_set = prepare_sample(sample, tokenizer, max_length=512, mask_inputs= True)\n",
    "#     if sample_set == None:\n",
    "#         continue\n",
    "#     else:\n",
    "#         train_sample_set.append(sample_set)\n",
    "#     torch.save(train_sample_set, training_file_path.parent / \"train_temp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52b9181-2a63-4b60-af19-816f7a2ea5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " dict_keys(['context', 'question', 'answer', 'input_ids', 'labels']),\n",
       " torch.Size([1, 157]),\n",
       " torch.Size([1, 157]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_datasets(data_dir):\n",
    "    train_data = torch.load(os.path.join(data_dir, \"train_temp.pt\"))\n",
    "    # val_data = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "    # return train_data, val_data\n",
    "    return train_data\n",
    "\n",
    "\n",
    "data_dir = \"/data/user-data/sa25729/lit_llama_qa/lit-llama-qa/data/\"\n",
    "train_data = load_datasets(data_dir)\n",
    "len(train_data), train_data[0].keys(), train_data[0]['input_ids'].size(), train_data[0]['labels'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c053e32f-2f93-492f-9abe-755676c39869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,  8458, 31922, 19775, 31871,    13,   261,\n",
       "            266,   473,   811,   287,   600,   281,     2]]),\n",
       " tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     1, 16121,\n",
       "          12945, 31871,    13,  3272,   397, 13357, 10700,  3984,   269,  1735,\n",
       "           3984,  3521,    13,    13,  8458, 31922, 19775, 31871,    13,   261,\n",
       "            266,   473,   811,   287,   600,   281,     2]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['labels'].type(torch.int64), train_data[0]['input_ids'].type(torch.int64),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c69b194-1da8-426d-9f93-fe154b3eaf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ix tensor([19])\n",
      "input_ids 1 torch.Size([1, 157])\n",
      "labels 1 torch.Size([1, 157])\n",
      "max_len 1\n",
      "x torch.Size([1, 1, 157])\n",
      "x torch.Size([1, 1, 157])\n"
     ]
    }
   ],
   "source": [
    "micro_batch_size =1\n",
    "\n",
    "def get_batch(data: list):\n",
    "    ix = torch.randint(len(data)-1, (micro_batch_size,))\n",
    "    print('ix', ix) \n",
    "    \n",
    "    input_ids = [data[i][\"input_ids\"].type(torch.float32) for i in ix] #torch.int64\n",
    "    labels = [data[i][\"labels\"].type(torch.int64) for i in ix]\n",
    "\n",
    "    # print('input_ids',len(input_ids) ,input_ids[0].size(), input_ids[1].size(), input_ids[2].size(), input_ids[3].size(), input_ids[4].size())\n",
    "    # print('labels', len(labels),labels[0].size(), labels[1].size(), labels[2].size(), labels[3].size(), labels[4].size())\n",
    "    \n",
    "    print('input_ids',len(input_ids) ,input_ids[0].size()) \n",
    "    print('labels', len(labels),labels[0].size())\n",
    "\n",
    "    # print('input_ids',input_ids ,input_ids[0].size()) \n",
    "    # print('labels', labels,labels[0].size())\n",
    "\n",
    "    max_len = max(len(s) for s in input_ids)\n",
    "    print('max_len', max_len)\n",
    "\n",
    "    def pad_right(x, pad_id):\n",
    "        # pad right based on the longest sequence\n",
    "        n = max_len - len(x)\n",
    "        return torch.cat((x, torch.full((n,), pad_id, dtype=x.dtype)))\n",
    "\n",
    "    x = torch.stack([pad_right(x, pad_id=0) for x in input_ids])\n",
    "    y = torch.stack([pad_right(x, pad_id=-1) for x in labels])\n",
    "    # x, y = fabric.to_device((x.pin_memory(), y.pin_memory()))\n",
    "\n",
    "    print('x', x.size())\n",
    "    print('x', y.size())\n",
    "    # print('x',len(x) ,x[0].size(), x[1].size(), x[2].size(), x[3].size(), x[4].size())\n",
    "    # print('y', len(y),y[0].size(), y[1].size(), y[2].size(), y[3].size(), y[4].size())\n",
    "    return x, y\n",
    "\n",
    "input_ids, targets = get_batch(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc23742-23d5-4293-a0b4-b8028797d1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.3019e-02,  3.0004e-01, -2.1092e-01,  2.7332e-01,  5.6172e-02,\n",
       "            3.1421e-01,  2.0004e-01, -3.9866e-01, -3.6370e-01,  3.8636e-03,\n",
       "            4.6104e-01, -3.8067e-01, -2.3545e-01,  1.6375e-01,  2.8970e-01,\n",
       "            5.8310e-01,  2.5817e-01,  2.2851e-01,  1.7788e-01, -3.9559e-01,\n",
       "           -8.2401e-02, -2.1959e-02, -7.0833e-02,  1.3922e-01,  2.1348e-01,\n",
       "            2.0838e-01, -5.1258e-02,  2.0160e-01, -4.7469e-01, -2.2535e-01,\n",
       "            3.8030e-02,  2.6900e-01, -7.5497e-01,  2.4113e-01,  1.7330e-01,\n",
       "           -5.0797e-01, -4.2867e-01,  5.5541e-02,  4.3060e-01,  1.2487e-01,\n",
       "            6.1188e-02,  3.0710e-01,  2.7663e-01,  6.8004e-01, -1.5852e-01,\n",
       "           -1.0050e-01, -4.2995e-01,  3.5580e-01,  1.6571e-01, -1.7752e-01,\n",
       "            6.5570e-02,  5.1389e-02, -5.8529e-01, -4.5564e-02, -1.8349e-01,\n",
       "           -9.3535e-02, -1.9400e-01, -4.8248e-02, -2.2423e-01,  7.4156e-02,\n",
       "            1.8327e-01,  6.2321e-03, -1.2860e-01,  1.9064e-01,  1.8702e-01,\n",
       "           -2.1854e-01, -2.1911e-01,  1.2040e-02, -2.6249e-01,  2.3878e-01,\n",
       "            3.3255e-01, -1.6166e-01, -4.1372e-01,  4.4802e-01, -4.2859e-02,\n",
       "           -2.8003e-01,  7.4173e-02,  5.5026e-01,  2.3861e-01, -2.4945e-01,\n",
       "           -3.2295e-01,  3.6668e-01,  3.6787e-01, -5.7551e-01,  5.8144e-01,\n",
       "            2.0864e-01,  3.9970e-01,  1.0815e-01, -3.4487e-01,  6.3543e-02,\n",
       "           -5.7960e-01, -2.0752e-02, -7.5259e-02, -5.1360e-01, -1.2427e-01,\n",
       "            1.6584e-01,  1.2026e-01, -3.0948e-01,  1.5027e-03, -2.0841e-01,\n",
       "           -8.5050e-02, -3.0283e-02,  3.5027e-02,  3.4271e-01,  2.6785e-01,\n",
       "           -1.9123e-01, -2.2934e-01,  5.0124e-01, -2.5555e-01,  2.1652e-01,\n",
       "            1.9129e-01,  9.3096e-02,  2.6810e-01, -2.5456e-01, -7.1693e-03,\n",
       "            1.7020e-02,  1.6260e-01,  3.0750e-01, -3.8769e-01, -3.8379e-01,\n",
       "           -1.7831e-01, -1.4266e-01,  4.8766e-02, -2.7246e-02,  1.8894e-01,\n",
       "            3.2189e-01,  2.0543e-01,  1.7839e-01,  1.0000e+00,  1.6121e+04,\n",
       "            1.2945e+04,  3.1871e+04,  1.3000e+01,  1.1906e+04,  3.2200e+02,\n",
       "            3.2200e+02,  1.3884e+04,  2.9200e+02,  8.0700e+02,  3.7400e+02,\n",
       "            1.4090e+04,  5.0200e+02,  3.1834e+04,  2.4353e+04,  1.3000e+01,\n",
       "            1.3000e+01,  8.4580e+03,  3.1922e+04,  1.9775e+04,  3.1871e+04,\n",
       "            1.3000e+01,  1.1340e+03,  1.2039e+04,  6.2200e+02,  8.7400e+02,\n",
       "            5.5490e+03,  2.0000e+00]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "              -1,    -1,    -1,    -1,    -1,    -1,  8458, 31922, 19775, 31871,\n",
       "              13,  1134, 12039,   622,   874,  5549,     2]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022889a-b784-4cbd-a31f-0c790525ac20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6471e90d-d6bf-47b8-bf0c-059413d1ec71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8b40a-d0d8-4773-98b2-e52d7197ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93dc7952-85aa-414c-924f-866d27b51e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/data/user-data/sa25729/lit_llama_qa/data/train_temp.pt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7347defa-4dae-4652-8b10-b6c5774f952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/lit_llama_qa/lit-llama-qa\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9efde8d-9e15-4039-956f-ec07d1848ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': tensor([[-0.1911, -0.2390, -0.1190, -0.4613, -0.2316,  0.0827, -0.2930,  0.2204,\n",
       "          -0.1746,  0.5785, -0.0113, -0.2014, -0.3710,  0.0827, -0.2906, -0.4311,\n",
       "          -0.1014,  0.1405, -0.0646, -0.2600, -0.4056,  0.2690, -0.1107,  0.3428,\n",
       "          -0.3609,  0.5873, -0.0112, -0.3733,  0.1605,  0.3588,  0.0437,  0.0793,\n",
       "          -0.1228, -0.0583, -0.0052,  0.0739, -0.0917,  0.2080, -0.3682, -0.1224,\n",
       "           0.3304, -0.1189, -0.1956, -0.4890,  0.1407, -0.0748, -0.0239, -0.1615,\n",
       "           0.1607, -0.2159, -0.1074,  0.1661,  0.1009,  0.2857,  0.4329,  0.5538,\n",
       "           0.2900,  0.0875,  0.1731,  0.0848,  0.2790,  0.2409,  0.0900, -0.3946,\n",
       "           0.1820,  0.0960, -0.1258, -0.3767,  0.0630, -0.1480,  0.1856,  0.1505,\n",
       "          -0.1298,  0.2001, -0.0829, -0.3554,  0.0613,  0.0326, -0.0554, -0.0528,\n",
       "          -0.1541, -0.1876,  0.2919,  0.0029,  0.1859,  0.1724,  0.2765, -0.1543,\n",
       "           0.1515,  0.4419,  0.4060,  0.3493,  0.0931,  0.0111,  0.1780, -0.2226,\n",
       "          -0.1765,  0.1955,  0.2687,  0.2444,  0.0006, -0.3001,  0.1520, -0.1158,\n",
       "           0.5657, -0.1804,  0.4615,  0.0728,  0.6125,  0.0852,  0.1875, -0.0050,\n",
       "           0.1899, -0.3990, -0.3495, -0.4494, -0.2694,  0.0164,  0.3064, -0.1475,\n",
       "          -0.2707,  0.1975,  0.0206, -0.4443,  0.1903,  0.1415,  0.0034, -0.2053]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'question': 'where are liver stem cells oval cells located',\n",
       " 'answer': 'in the canals of hering',\n",
       " 'input_ids': tensor([[-1.9109e-01, -2.3902e-01, -1.1902e-01, -4.6130e-01, -2.3164e-01,\n",
       "           8.2665e-02, -2.9302e-01,  2.2038e-01, -1.7456e-01,  5.7846e-01,\n",
       "          -1.1300e-02, -2.0143e-01, -3.7097e-01,  8.2741e-02, -2.9062e-01,\n",
       "          -4.3109e-01, -1.0141e-01,  1.4047e-01, -6.4559e-02, -2.6000e-01,\n",
       "          -4.0563e-01,  2.6899e-01, -1.1071e-01,  3.4279e-01, -3.6094e-01,\n",
       "           5.8733e-01, -1.1152e-02, -3.7328e-01,  1.6054e-01,  3.5877e-01,\n",
       "           4.3712e-02,  7.9252e-02, -1.2277e-01, -5.8282e-02, -5.1744e-03,\n",
       "           7.3861e-02, -9.1655e-02,  2.0804e-01, -3.6816e-01, -1.2239e-01,\n",
       "           3.3041e-01, -1.1888e-01, -1.9563e-01, -4.8904e-01,  1.4073e-01,\n",
       "          -7.4846e-02, -2.3851e-02, -1.6149e-01,  1.6067e-01, -2.1594e-01,\n",
       "          -1.0739e-01,  1.6606e-01,  1.0088e-01,  2.8571e-01,  4.3293e-01,\n",
       "           5.5383e-01,  2.9004e-01,  8.7515e-02,  1.7313e-01,  8.4847e-02,\n",
       "           2.7898e-01,  2.4089e-01,  8.9997e-02, -3.9460e-01,  1.8202e-01,\n",
       "           9.5965e-02, -1.2582e-01, -3.7668e-01,  6.2957e-02, -1.4799e-01,\n",
       "           1.8561e-01,  1.5051e-01, -1.2977e-01,  2.0010e-01, -8.2870e-02,\n",
       "          -3.5541e-01,  6.1293e-02,  3.2612e-02, -5.5391e-02, -5.2754e-02,\n",
       "          -1.5406e-01, -1.8761e-01,  2.9190e-01,  2.8709e-03,  1.8591e-01,\n",
       "           1.7240e-01,  2.7651e-01, -1.5426e-01,  1.5153e-01,  4.4186e-01,\n",
       "           4.0597e-01,  3.4929e-01,  9.3096e-02,  1.1132e-02,  1.7803e-01,\n",
       "          -2.2263e-01, -1.7655e-01,  1.9555e-01,  2.6872e-01,  2.4442e-01,\n",
       "           6.2327e-04, -3.0014e-01,  1.5201e-01, -1.1576e-01,  5.6569e-01,\n",
       "          -1.8044e-01,  4.6145e-01,  7.2844e-02,  6.1251e-01,  8.5223e-02,\n",
       "           1.8753e-01, -4.9789e-03,  1.8988e-01, -3.9902e-01, -3.4946e-01,\n",
       "          -4.4944e-01, -2.6940e-01,  1.6404e-02,  3.0640e-01, -1.4749e-01,\n",
       "          -2.7067e-01,  1.9754e-01,  2.0649e-02, -4.4425e-01,  1.9025e-01,\n",
       "           1.4147e-01,  3.3647e-03, -2.0529e-01,  1.0000e+00,  1.6121e+04,\n",
       "           1.2945e+04,  3.1871e+04,  1.3000e+01,  3.2720e+03,  3.9700e+02,\n",
       "           1.3357e+04,  1.0700e+04,  3.9840e+03,  2.6900e+02,  1.7350e+03,\n",
       "           3.9840e+03,  3.5210e+03,  1.3000e+01,  1.3000e+01,  8.4580e+03,\n",
       "           3.1922e+04,  1.9775e+04,  3.1871e+04,  1.3000e+01,  2.6100e+02,\n",
       "           2.6600e+02,  4.7300e+02,  8.1100e+02,  2.8700e+02,  6.0000e+02,\n",
       "           2.8100e+02,  2.0000e+00]], grad_fn=<TransposeBackward0>),\n",
       " 'labels': tensor([[-1.9109e-01, -2.3902e-01, -1.1902e-01, -4.6130e-01, -2.3164e-01,\n",
       "           8.2665e-02, -2.9302e-01,  2.2038e-01, -1.7456e-01,  5.7846e-01,\n",
       "          -1.1300e-02, -2.0143e-01, -3.7097e-01,  8.2741e-02, -2.9062e-01,\n",
       "          -4.3109e-01, -1.0141e-01,  1.4047e-01, -6.4559e-02, -2.6000e-01,\n",
       "          -4.0563e-01,  2.6899e-01, -1.1071e-01,  3.4279e-01, -3.6094e-01,\n",
       "           5.8733e-01, -1.1152e-02, -3.7328e-01,  1.6054e-01,  3.5877e-01,\n",
       "           4.3712e-02,  7.9252e-02, -1.2277e-01, -5.8282e-02, -5.1744e-03,\n",
       "           7.3861e-02, -9.1655e-02,  2.0804e-01, -3.6816e-01, -1.2239e-01,\n",
       "           3.3041e-01, -1.1888e-01, -1.9563e-01, -4.8904e-01,  1.4073e-01,\n",
       "          -7.4846e-02, -2.3851e-02, -1.6149e-01,  1.6067e-01, -2.1594e-01,\n",
       "          -1.0739e-01,  1.6606e-01,  1.0088e-01,  2.8571e-01,  4.3293e-01,\n",
       "           5.5383e-01,  2.9004e-01,  8.7515e-02,  1.7313e-01,  8.4847e-02,\n",
       "           2.7898e-01,  2.4089e-01,  8.9997e-02, -3.9460e-01,  1.8202e-01,\n",
       "           9.5965e-02, -1.2582e-01, -3.7668e-01,  6.2957e-02, -1.4799e-01,\n",
       "           1.8561e-01,  1.5051e-01, -1.2977e-01,  2.0010e-01, -8.2870e-02,\n",
       "          -3.5541e-01,  6.1293e-02,  3.2612e-02, -5.5391e-02, -5.2754e-02,\n",
       "          -1.5406e-01, -1.8761e-01,  2.9190e-01,  2.8709e-03,  1.8591e-01,\n",
       "           1.7240e-01,  2.7651e-01, -1.5426e-01,  1.5153e-01,  4.4186e-01,\n",
       "           4.0597e-01,  3.4929e-01,  9.3096e-02,  1.1132e-02,  1.7803e-01,\n",
       "          -2.2263e-01, -1.7655e-01,  1.9555e-01,  2.6872e-01,  2.4442e-01,\n",
       "           6.2327e-04, -3.0014e-01,  1.5201e-01, -1.1576e-01,  5.6569e-01,\n",
       "          -1.8044e-01,  4.6145e-01,  7.2844e-02,  6.1251e-01,  8.5223e-02,\n",
       "           1.8753e-01, -4.9789e-03,  1.8988e-01, -3.9902e-01, -3.4946e-01,\n",
       "          -4.4944e-01, -2.6940e-01,  1.6404e-02,  3.0640e-01, -1.4749e-01,\n",
       "          -2.7067e-01,  1.9754e-01,  2.0649e-02, -4.4425e-01,  1.9025e-01,\n",
       "           1.4147e-01,  3.3647e-03, -2.0529e-01,  1.0000e+00,  1.6121e+04,\n",
       "           1.2945e+04,  3.1871e+04,  1.3000e+01,  3.2720e+03,  3.9700e+02,\n",
       "           1.3357e+04,  1.0700e+04,  3.9840e+03,  2.6900e+02,  1.7350e+03,\n",
       "           3.9840e+03,  3.5210e+03,  1.3000e+01,  1.3000e+01,  8.4580e+03,\n",
       "           3.1922e+04,  1.9775e+04,  3.1871e+04,  1.3000e+01,  2.6100e+02,\n",
       "           2.6600e+02,  4.7300e+02,  8.1100e+02,  2.8700e+02,  6.0000e+02,\n",
       "           2.8100e+02,  2.0000e+00]], grad_fn=<CopySlices>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "IGNORE_INDEX = -1\n",
    "tokenizer_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "sample_set = prepare_sample(sample, tokenizer, max_length=512, mask_inputs= True)\n",
    "\n",
    "\n",
    "sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be1168d3-049d-45e7-b8bd-a663c5c82d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prompt_qa_(example, item):\n",
    "    \"\"\"Generates a standardized message to prompt the model with an instruction, optional input and a\n",
    "    'response' field.\"\"\"\n",
    "    if item == \"question\": \n",
    "        return f\"### Question:\\n{example[item]}\\n\\n### Answer:\\n\"\n",
    "    else:\n",
    "        return example[item]\n",
    "    # return f\"### Context:\\n{example['context']}\\n\\n### Question:\\n{example['question']}\\n\\n### Answer:\\n\"\n",
    "\n",
    "temp = generate_prompt_qa_(train_dataset[0], item = 'context')\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a262a-f7d6-45fe-81ec-5d3c653e289d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f13492-076d-4fca-94ef-b09ee1b31c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b6ca9-ebf0-45b3-9bf1-c3c9e42c8470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "915862da-1336-4a1f-8257-1d4849a38f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_prompt_and_response ### Question:\n",
      "where are liver stem cells oval cells located\n",
      "\n",
      "### Answer:\n",
      "in the canals of hering\n",
      "encoded_full_prompt_and_response torch.Size([29]) tensor([    1, 16121, 12945, 31871,    13,  3272,   397, 13357, 10700,  3984,\n",
      "          269,  1735,  3984,  3521,    13,    13,  8458, 31922, 19775, 31871,\n",
      "           13,   261,   266,   473,   811,   287,   600,   281,     2],\n",
      "       dtype=torch.int32)\n",
      "x torch.Size([128, 1])\n",
      "y torch.Size([29, 1]) tensor([[    1],\n",
      "        [16121],\n",
      "        [12945],\n",
      "        [31871],\n",
      "        [   13],\n",
      "        [ 3272],\n",
      "        [  397],\n",
      "        [13357],\n",
      "        [10700],\n",
      "        [ 3984],\n",
      "        [  269],\n",
      "        [ 1735],\n",
      "        [ 3984],\n",
      "        [ 3521],\n",
      "        [   13],\n",
      "        [   13],\n",
      "        [ 8458],\n",
      "        [31922],\n",
      "        [19775],\n",
      "        [31871],\n",
      "        [   13],\n",
      "        [  261],\n",
      "        [  266],\n",
      "        [  473],\n",
      "        [  811],\n",
      "        [  287],\n",
      "        [  600],\n",
      "        [  281],\n",
      "        [    2]], dtype=torch.int32)\n",
      "torch.Size([128, 1]) torch.Size([29, 1]) torch.Size([1, 157]) tensor([[-1.0553e-01,  2.7340e-01,  1.2622e-01, -2.3473e-01,  2.8224e-03,\n",
      "          4.0047e-03, -1.9141e-01,  4.6475e-02,  2.0619e-01,  1.0542e-01,\n",
      "         -2.3641e-01,  3.5186e-01, -1.4731e-01,  2.5889e-02,  8.2009e-02,\n",
      "          1.0870e-02,  1.3855e-02,  3.0666e-01, -2.5922e-01,  8.4844e-02,\n",
      "          2.5139e-01,  3.5179e-01,  4.4966e-01, -2.9509e-01, -3.1299e-01,\n",
      "         -1.1183e-01, -9.4091e-02,  3.0542e-01, -5.0706e-01,  6.8690e-02,\n",
      "         -1.3625e-01,  4.2783e-01, -4.0312e-01, -1.6762e-01,  7.1427e-03,\n",
      "          5.1982e-02,  3.7525e-01,  1.1412e-01,  3.7186e-01, -3.1479e-01,\n",
      "         -2.3052e-01, -5.3772e-02, -9.8913e-02,  2.5166e-01,  1.0170e-01,\n",
      "         -2.8236e-01,  1.5969e-01, -2.7109e-01,  1.0966e-01,  2.3179e-01,\n",
      "          1.0436e-01,  1.7429e-01,  3.4985e-01,  2.4119e-01, -1.4901e-01,\n",
      "          1.8322e-01,  1.1825e-01, -7.9911e-03, -1.8182e-01, -1.1106e-01,\n",
      "          4.6892e-01, -1.0796e-01, -1.9389e-01, -3.9273e-01,  4.6294e-02,\n",
      "         -1.9126e-02, -6.0484e-02, -1.4997e-01,  2.3212e-01,  1.6845e-01,\n",
      "         -1.7038e-01,  1.6702e-01, -5.3448e-01,  2.4721e-01,  6.8710e-01,\n",
      "         -9.6914e-02,  1.6175e-01, -3.0837e-01,  2.1110e-01, -9.2270e-02,\n",
      "          4.6033e-02,  3.7856e-01, -3.2049e-01, -4.8909e-02,  1.9744e-01,\n",
      "         -6.9543e-02,  1.4147e-01,  1.2382e-01,  3.3801e-01, -5.3290e-02,\n",
      "         -1.0268e-01,  1.1139e-01, -1.3389e-01,  2.8119e-01, -1.5822e-01,\n",
      "         -3.1714e-01,  1.1253e-01, -4.5929e-01,  4.8781e-03,  7.6088e-02,\n",
      "          3.4202e-01, -2.5727e-01,  1.0119e-01, -1.8956e-01,  1.8952e-02,\n",
      "          2.0349e-01,  5.0338e-02,  2.5173e-01,  1.3996e-01, -2.7599e-01,\n",
      "         -3.6901e-01,  2.1835e-01, -3.7168e-01,  1.5815e-01, -1.1161e-01,\n",
      "         -5.4547e-01, -2.0514e-01,  1.9701e-01, -2.0463e-02,  1.0384e-01,\n",
      "          7.9810e-02, -3.6093e-02, -1.5675e-02,  2.4242e-01,  1.0153e-01,\n",
      "          4.5233e-01,  2.1300e-01,  4.4744e-01,  1.0000e+00,  1.6121e+04,\n",
      "          1.2945e+04,  3.1871e+04,  1.3000e+01,  3.2720e+03,  3.9700e+02,\n",
      "          1.3357e+04,  1.0700e+04,  3.9840e+03,  2.6900e+02,  1.7350e+03,\n",
      "          3.9840e+03,  3.5210e+03,  1.3000e+01,  1.3000e+01,  8.4580e+03,\n",
      "          3.1922e+04,  1.9775e+04,  3.1871e+04,  1.3000e+01,  2.6100e+02,\n",
      "          2.6600e+02,  4.7300e+02,  8.1100e+02,  2.8700e+02,  6.0000e+02,\n",
      "          2.8100e+02,  2.0000e+00]], grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': tensor([[-0.1055,  0.2734,  0.1262, -0.2347,  0.0028,  0.0040, -0.1914,  0.0465,\n",
       "           0.2062,  0.1054, -0.2364,  0.3519, -0.1473,  0.0259,  0.0820,  0.0109,\n",
       "           0.0139,  0.3067, -0.2592,  0.0848,  0.2514,  0.3518,  0.4497, -0.2951,\n",
       "          -0.3130, -0.1118, -0.0941,  0.3054, -0.5071,  0.0687, -0.1363,  0.4278,\n",
       "          -0.4031, -0.1676,  0.0071,  0.0520,  0.3752,  0.1141,  0.3719, -0.3148,\n",
       "          -0.2305, -0.0538, -0.0989,  0.2517,  0.1017, -0.2824,  0.1597, -0.2711,\n",
       "           0.1097,  0.2318,  0.1044,  0.1743,  0.3498,  0.2412, -0.1490,  0.1832,\n",
       "           0.1183, -0.0080, -0.1818, -0.1111,  0.4689, -0.1080, -0.1939, -0.3927,\n",
       "           0.0463, -0.0191, -0.0605, -0.1500,  0.2321,  0.1685, -0.1704,  0.1670,\n",
       "          -0.5345,  0.2472,  0.6871, -0.0969,  0.1618, -0.3084,  0.2111, -0.0923,\n",
       "           0.0460,  0.3786, -0.3205, -0.0489,  0.1974, -0.0695,  0.1415,  0.1238,\n",
       "           0.3380, -0.0533, -0.1027,  0.1114, -0.1339,  0.2812, -0.1582, -0.3171,\n",
       "           0.1125, -0.4593,  0.0049,  0.0761,  0.3420, -0.2573,  0.1012, -0.1896,\n",
       "           0.0190,  0.2035,  0.0503,  0.2517,  0.1400, -0.2760, -0.3690,  0.2183,\n",
       "          -0.3717,  0.1582, -0.1116, -0.5455, -0.2051,  0.1970, -0.0205,  0.1038,\n",
       "           0.0798, -0.0361, -0.0157,  0.2424,  0.1015,  0.4523,  0.2130,  0.4474]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'question': 'where are liver stem cells oval cells located',\n",
       " 'answer': 'in the canals of hering',\n",
       " 'input_ids': tensor([[-1.0553e-01,  2.7340e-01,  1.2622e-01, -2.3473e-01,  2.8224e-03,\n",
       "           4.0047e-03, -1.9141e-01,  4.6475e-02,  2.0619e-01,  1.0542e-01,\n",
       "          -2.3641e-01,  3.5186e-01, -1.4731e-01,  2.5889e-02,  8.2009e-02,\n",
       "           1.0870e-02,  1.3855e-02,  3.0666e-01, -2.5922e-01,  8.4844e-02,\n",
       "           2.5139e-01,  3.5179e-01,  4.4966e-01, -2.9509e-01, -3.1299e-01,\n",
       "          -1.1183e-01, -9.4091e-02,  3.0542e-01, -5.0706e-01,  6.8690e-02,\n",
       "          -1.3625e-01,  4.2783e-01, -4.0312e-01, -1.6762e-01,  7.1427e-03,\n",
       "           5.1982e-02,  3.7525e-01,  1.1412e-01,  3.7186e-01, -3.1479e-01,\n",
       "          -2.3052e-01, -5.3772e-02, -9.8913e-02,  2.5166e-01,  1.0170e-01,\n",
       "          -2.8236e-01,  1.5969e-01, -2.7109e-01,  1.0966e-01,  2.3179e-01,\n",
       "           1.0436e-01,  1.7429e-01,  3.4985e-01,  2.4119e-01, -1.4901e-01,\n",
       "           1.8322e-01,  1.1825e-01, -7.9911e-03, -1.8182e-01, -1.1106e-01,\n",
       "           4.6892e-01, -1.0796e-01, -1.9389e-01, -3.9273e-01,  4.6294e-02,\n",
       "          -1.9126e-02, -6.0484e-02, -1.4997e-01,  2.3212e-01,  1.6845e-01,\n",
       "          -1.7038e-01,  1.6702e-01, -5.3448e-01,  2.4721e-01,  6.8710e-01,\n",
       "          -9.6914e-02,  1.6175e-01, -3.0837e-01,  2.1110e-01, -9.2270e-02,\n",
       "           4.6033e-02,  3.7856e-01, -3.2049e-01, -4.8909e-02,  1.9744e-01,\n",
       "          -6.9543e-02,  1.4147e-01,  1.2382e-01,  3.3801e-01, -5.3290e-02,\n",
       "          -1.0268e-01,  1.1139e-01, -1.3389e-01,  2.8119e-01, -1.5822e-01,\n",
       "          -3.1714e-01,  1.1253e-01, -4.5929e-01,  4.8781e-03,  7.6088e-02,\n",
       "           3.4202e-01, -2.5727e-01,  1.0119e-01, -1.8956e-01,  1.8952e-02,\n",
       "           2.0349e-01,  5.0338e-02,  2.5173e-01,  1.3996e-01, -2.7599e-01,\n",
       "          -3.6901e-01,  2.1835e-01, -3.7168e-01,  1.5815e-01, -1.1161e-01,\n",
       "          -5.4547e-01, -2.0514e-01,  1.9701e-01, -2.0463e-02,  1.0384e-01,\n",
       "           7.9810e-02, -3.6093e-02, -1.5675e-02,  2.4242e-01,  1.0153e-01,\n",
       "           4.5233e-01,  2.1300e-01,  4.4744e-01,  1.0000e+00,  1.6121e+04,\n",
       "           1.2945e+04,  3.1871e+04,  1.3000e+01,  3.2720e+03,  3.9700e+02,\n",
       "           1.3357e+04,  1.0700e+04,  3.9840e+03,  2.6900e+02,  1.7350e+03,\n",
       "           3.9840e+03,  3.5210e+03,  1.3000e+01,  1.3000e+01,  8.4580e+03,\n",
       "           3.1922e+04,  1.9775e+04,  3.1871e+04,  1.3000e+01,  2.6100e+02,\n",
       "           2.6600e+02,  4.7300e+02,  8.1100e+02,  2.8700e+02,  6.0000e+02,\n",
       "           2.8100e+02,  2.0000e+00]], grad_fn=<TransposeBackward0>),\n",
       " 'labels': tensor([[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1.]], grad_fn=<FillBackward3>)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sample_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e5292-b827-490e-b487-2b17954c3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer_path = Path(\"checkpoints/lit-llama/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "train_sample_set = []\n",
    "for sample in tqdm(train_dataset):\n",
    "        sample_set = prepare_sample(sample, tokenizer, max_seq_length=512, mask_inputs= True)\n",
    "        if sample_set == None:\n",
    "            continue\n",
    "        else:\n",
    "            train_sample_set.append(sample_set)\n",
    "    torch.save(train_sample_set, training_file_path.parent / \"train_temp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cad177-55cd-48e3-925b-d7abc46b4b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9225ced-a49a-4ea4-afc0-32be59b75a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eb13c-c8f1-42d6-aa8d-54e2ac80e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = {'pixel_values':0}\n",
    "inputs1['pixel_values'] = train_loader.dataset[5][0].unsqueeze(0)\n",
    "print(inputs1['pixel_values'].shape)\n",
    "image_features = model.get_image_features(**inputs1)\n",
    "print(image_features.shape)\n",
    "\n",
    "proj = img_projection((512, 128))\n",
    "image_features_proj = proj(image_features)\n",
    "print(image_features_proj.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
